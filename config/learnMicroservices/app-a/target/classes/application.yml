# App A - Application Configuration
# Main application properties (overridden by Config Server during startup)

spring:
  application:
    name: app-a
  
  # IMPORTANT: Define config import here as well (IntelliJ compatibility)
  config:
    import: optional:configserver:http://localhost:8888
  
  cloud:
    config:
      enabled: true
      uri: http://localhost:8888
      fail-fast: false
      retry:
        initial-interval: 1000
        max-interval: 2000
        max-attempts: 6
  
  # ===== DISTRIBUTED TRACING CONFIGURATION (Phase 3) =====
  sleuth:
    enabled: true
    sampler:
      probability: 1.0
  
  # Enable trace ID propagation in HTTP headers (B3 format)
  # This sends trace ID to other services via HTTP headers
  # So App B will see the SAME trace ID
  tracing:
    propagation:
      type: b3

# Default values (overridden by Config Server)
# ⚠️ COMMENTED OUT - Testing if config server is being used
# app:
#   name: "App A"
#   description: "App A Microservice"
#   version: "1.0.0"
#   environment: "default"
#   timeout: 30000

# Server port (can be overridden)
server:
  port: 8080

# Management
management:
  endpoints:
    web:
      exposure:
        include: health,info,env,configprops,refresh,circuitbreakers,retries,bulkheads,timelimiters,ratelimiters,metrics
  endpoint:
    health:
      show-details: always
      show-components: always
  health:
    circuitbreakers:
      enabled: true
    ratelimiters:
      enabled: true

# ============================================================
# PHASE 4: RESILIENCE4J CONFIGURATION
# ============================================================
# 
# WHY RESILIENCE4J?
# -----------------
# In microservices, failures are EXPECTED, not exceptional.
# Without resilience patterns:
#   - One slow service → all services slow (cascade)
#   - One failing service → all services fail (cascade)
#   - Thread pools exhausted → system freeze
#
# Resilience4j provides 5 key patterns:
#   1. Circuit Breaker - Stop calling failing services
#   2. Retry - Retry transient failures
#   3. Timeout - Don't wait forever
#   4. Bulkhead - Isolate thread pools
#   5. Rate Limiter - Control request rate
#
resilience4j:
  # ============================================================
  # CIRCUIT BREAKER PATTERN
  # ============================================================
  # 
  # WHAT IS A CIRCUIT BREAKER?
  # --------------------------
  # Like an electrical circuit breaker in your home:
  #   - Normal: Current flows (CLOSED state)
  #   - Overload: Breaker trips (OPEN state)
  #   - Testing: Try again (HALF_OPEN state)
  #
  # THREE STATES:
  #   CLOSED: Normal operation, requests flow through
  #   OPEN: Fail fast, don't send requests (service is down)
  #   HALF_OPEN: Test if service recovered
  #
  # WHY USE IT?
  #   - Prevents cascading failures
  #   - Fails fast (< 100ms) instead of timeout (30s)
  #   - Auto-recovers when service comes back
  #
  circuitbreaker:
    instances:
      # Circuit breaker for calls to App B
      appBCircuitBreaker:
        # Register health indicator for /actuator/health
        registerHealthIndicator: true
        
        # SLIDING WINDOW: Analyze last 10 calls to calculate failure rate
        # Why 10? Small enough to react quickly, big enough for accuracy
        slidingWindowSize: 10
        slidingWindowType: COUNT_BASED
        
        # MINIMUM CALLS: Need at least 5 calls before opening circuit
        # Why 5? Prevents false alarms on few requests
        minimumNumberOfCalls: 5
        
        # FAILURE THRESHOLD: Open circuit if 50% of calls fail
        # Why 50%? Balance between sensitivity and stability
        failureRateThreshold: 50
        
        # SLOW CALL THRESHOLD: Also monitor slow calls (not just errors)
        slowCallRateThreshold: 50
        slowCallDurationThreshold: 2s
        
        # WAIT IN OPEN STATE: Wait 30 seconds before trying again
        # Why 30s? Enough time for service to recover, not too long for user
        waitDurationInOpenState: 30s
        
        # HALF_OPEN STATE: Allow 3 test calls to check recovery
        # Why 3? Enough to confirm recovery, not too many if still failing
        permittedNumberOfCallsInHalfOpenState: 3
        
        # AUTO TRANSITION: Automatically test after wait duration
        automaticTransitionFromOpenToHalfOpenEnabled: true
        
        # WHAT TO COUNT AS FAILURE: These exceptions trigger circuit breaker
        recordExceptions:
          - java.io.IOException
          - java.net.SocketTimeoutException
          - java.net.ConnectException
          - org.springframework.web.client.ResourceAccessException
          - feign.FeignException
        
        # WHAT TO IGNORE: These don't count toward failure rate
        ignoreExceptions:
          - com.masterclass.appa.exception.BusinessException

  # ============================================================
  # RETRY PATTERN
  # ============================================================
  #
  # WHAT IS RETRY?
  # --------------
  # Some failures are TRANSIENT (temporary):
  #   - Brief network glitch
  #   - Service temporarily busy
  #   - Database connection spike
  #
  # Waiting a moment and retrying often works!
  #
  # EXPONENTIAL BACKOFF:
  #   Attempt 1: Wait 500ms
  #   Attempt 2: Wait 1000ms (doubled)
  #   Attempt 3: Wait 2000ms (doubled again)
  #   
  # WHY BACKOFF? Gives service time to recover
  # WHY EXPONENTIAL? Prevents "thundering herd" problem
  #
  retry:
    instances:
      appBRetry:
        # MAX ATTEMPTS: Try 3 times total (1 original + 2 retries)
        # Why 3? Enough for transient failures, not too many delays
        maxAttempts: 3
        
        # INITIAL WAIT: 500ms before first retry
        # Why 500ms? Enough for brief network issues
        waitDuration: 5000ms
        
        # EXPONENTIAL BACKOFF: Enable increasing wait times
        enableExponentialBackoff: true
        exponentialBackoffMultiplier: 2
        
        # MAX WAIT: Cap at 5 seconds (even with exponential)
        # Why 5s? Don't make user wait too long
        exponentialMaxWaitDuration: 5s
        
        # RETRY ON: Only retry these exceptions (transient failures)
        retryExceptions:
          - java.io.IOException
          - java.net.SocketTimeoutException
          - java.net.ConnectException
          - feign.RetryableException
          - feign.FeignException$ServiceUnavailable  # 503 - Service down
          - feign.FeignException$InternalServerError  # 500 - Internal error
          - feign.FeignException$GatewayTimeout       # 504 - Gateway timeout
        
        # DON'T RETRY: These are permanent failures (client errors)
        ignoreExceptions:
          - feign.FeignException$BadRequest   # 400 - Bad request
          - feign.FeignException$NotFound     # 404 - Not found
          - feign.FeignException$Forbidden    # 403 - Forbidden
          - feign.FeignException$Unauthorized # 401 - Unauthorized

  # ============================================================
  # TIMEOUT (TIME LIMITER) PATTERN
  # ============================================================
  #
  # WHY TIMEOUT?
  # ------------
  # Without timeout:
  #   - Service hangs → your thread hangs → all threads hang
  #   - User waits 30+ seconds → bad experience
  #   - Thread pool exhausted → system freeze
  #
  # With timeout:
  #   - Service hangs → fail fast after 3s
  #   - User gets response quickly (even if error)
  #   - Threads freed up for other requests
  #
  timelimiter:
    instances:
      appBTimeLimiter:
        # TIMEOUT: Fail if request takes > 3 seconds
        # Why 3s? App B should respond in < 1s normally
        # 3s = safety margin for slow queries
        timeoutDuration: 30ms
        
        # CANCEL: Stop the request if timeout reached
        cancelRunningFuture: true

  # ============================================================
  # BULKHEAD PATTERN
  # ============================================================
  #
  # WHAT IS A BULKHEAD?
  # -------------------
  # Like compartments in a ship:
  #   - One leak → seal that compartment
  #   - Ship stays afloat (other compartments safe)
  #
  # In microservices:
  #   - One slow service → isolate its thread pool
  #   - Other services unaffected
  #
  # EXAMPLE:
  #   Without bulkhead: 50 threads shared
  #     - App B slow → 45 threads waiting
  #     - Only 5 threads for everything else!
  #   
  #   With bulkhead: 10 threads for App B calls
  #     - App B slow → only 10 threads affected
  #     - 40 threads for other operations!
  #
  bulkhead:
    instances:
      appBBulkhead:
        # MAX CONCURRENT: Only 10 simultaneous calls to App B
        # Why 10? Protects App B from overload, protects our threads
        maxConcurrentCalls: 10
        
        # MAX WAIT: Wait max 500ms if all threads busy
        # Why 500ms? Fast fail if bulkhead full
        maxWaitDuration: 500ms

  # ============================================================
  # RATE LIMITER PATTERN
  # ============================================================
  #
  # WHY RATE LIMITING?
  # ------------------
  # Protects services from being overwhelmed:
  #   - Too many requests → service crashes
  #   - Rate limit → controlled flow
  #
  # EXAMPLE: 100 requests per second limit
  #   - Normal traffic (50 req/s) → all pass
  #   - Spike (200 req/s) → 100 pass, 100 rejected
  #   - Service stays healthy!
  #
  ratelimiter:
    instances:
      appBRateLimiter:
        # LIMIT: Max 100 requests per second
        limitForPeriod: 1
        limitRefreshPeriod: 1s
        
        # TIMEOUT: Wait max 500ms if rate limited
        timeoutDuration: 500ms

# Logging
logging:
  level:
    root: INFO
    org.springframework.web: DEBUG
    org.springframework.cloud: DEBUG
    com.masterclass: DEBUG
  pattern:
    console: "%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - [TRACE: %X{traceId}] %msg%n"

# Eureka Client Configuration
eureka:
  client:
    service-url:
      defaultZone: http://localhost:8761/eureka/
    register-with-eureka: true
    fetch-registry: true
  instance:
    prefer-ip-address: true
    ip-address: 127.0.0.1
